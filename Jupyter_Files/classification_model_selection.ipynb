{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "227bce34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1f2526d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Title</th>\n",
       "      <th>Product</th>\n",
       "      <th>Category</th>\n",
       "      <th>Geography</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>Reference Period</th>\n",
       "      <th>Release Date</th>\n",
       "      <th>Table No</th>\n",
       "      <th>Download URL</th>\n",
       "      <th>Data Source</th>\n",
       "      <th>Description</th>\n",
       "      <th>id</th>\n",
       "      <th>search_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>28</td>\n",
       "      <td>Statement 1: Average Monthly Per Capita Consum...</td>\n",
       "      <td>HCES</td>\n",
       "      <td>National Sample Surveu</td>\n",
       "      <td>All India</td>\n",
       "      <td>As and when requirement arises</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>31-12-2024</td>\n",
       "      <td>HCESAFY24001ANN</td>\n",
       "      <td>https://esankhyiki.mospi.gov.in/datacatalogue/...</td>\n",
       "      <td>National Sample Survey Office</td>\n",
       "      <td>The Household Consumption Expenditure Survey (...</td>\n",
       "      <td>28</td>\n",
       "      <td>statement 1 average monthly per capita consump...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>28</td>\n",
       "      <td>command 1 : Average Monthly Per Capita Consump...</td>\n",
       "      <td>HCES</td>\n",
       "      <td>National Sample Surveu</td>\n",
       "      <td>All India</td>\n",
       "      <td>As and when requirement arises</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>31-12-2024</td>\n",
       "      <td>HCESAFY24001ANN</td>\n",
       "      <td>https://esankhyiki.mospi.gov.in/datacatalogue/...</td>\n",
       "      <td>National Sample Survey Office</td>\n",
       "      <td>The Household Consumption Expenditure Survey (...</td>\n",
       "      <td>28</td>\n",
       "      <td>assertion 1 intermediate monthly per capita pu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>28</td>\n",
       "      <td>assertion 1 : Average Monthly Per Capita Consu...</td>\n",
       "      <td>HCES</td>\n",
       "      <td>National Sample Surveu</td>\n",
       "      <td>All India</td>\n",
       "      <td>As and when requirement arises</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>31-12-2024</td>\n",
       "      <td>HCESAFY24001ANN</td>\n",
       "      <td>https://esankhyiki.mospi.gov.in/datacatalogue/...</td>\n",
       "      <td>National Sample Survey Office</td>\n",
       "      <td>The Household Consumption Expenditure Survey (...</td>\n",
       "      <td>28</td>\n",
       "      <td>command 1 ordinary monthly per capita uptake s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>28</td>\n",
       "      <td>affirmation 1 : Average Monthly Per Capita Con...</td>\n",
       "      <td>HCES</td>\n",
       "      <td>National Sample Surveu</td>\n",
       "      <td>All India</td>\n",
       "      <td>As and when requirement arises</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>31-12-2024</td>\n",
       "      <td>HCESAFY24001ANN</td>\n",
       "      <td>https://esankhyiki.mospi.gov.in/datacatalogue/...</td>\n",
       "      <td>National Sample Survey Office</td>\n",
       "      <td>The Household Consumption Expenditure Survey (...</td>\n",
       "      <td>28</td>\n",
       "      <td>assertion 1 mean monthly per capita intake exp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>28</td>\n",
       "      <td>statement 1 : Average Monthly Per Capita Consu...</td>\n",
       "      <td>HCES</td>\n",
       "      <td>National Sample Surveu</td>\n",
       "      <td>All India</td>\n",
       "      <td>As and when requirement arises</td>\n",
       "      <td>2023-24</td>\n",
       "      <td>31-12-2024</td>\n",
       "      <td>HCESAFY24001ANN</td>\n",
       "      <td>https://esankhyiki.mospi.gov.in/datacatalogue/...</td>\n",
       "      <td>National Sample Survey Office</td>\n",
       "      <td>The Household Consumption Expenditure Survey (...</td>\n",
       "      <td>28</td>\n",
       "      <td>affirmation 1 mediocre monthly per capita usan...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                              Title Product  \\\n",
       "0          28  Statement 1: Average Monthly Per Capita Consum...    HCES   \n",
       "1          28  command 1 : Average Monthly Per Capita Consump...    HCES   \n",
       "2          28  assertion 1 : Average Monthly Per Capita Consu...    HCES   \n",
       "3          28  affirmation 1 : Average Monthly Per Capita Con...    HCES   \n",
       "4          28  statement 1 : Average Monthly Per Capita Consu...    HCES   \n",
       "\n",
       "                 Category  Geography                       Frequency  \\\n",
       "0  National Sample Surveu  All India  As and when requirement arises   \n",
       "1  National Sample Surveu  All India  As and when requirement arises   \n",
       "2  National Sample Surveu  All India  As and when requirement arises   \n",
       "3  National Sample Surveu  All India  As and when requirement arises   \n",
       "4  National Sample Surveu  All India  As and when requirement arises   \n",
       "\n",
       "  Reference Period Release Date         Table No  \\\n",
       "0          2023-24   31-12-2024  HCESAFY24001ANN   \n",
       "1          2023-24   31-12-2024  HCESAFY24001ANN   \n",
       "2          2023-24   31-12-2024  HCESAFY24001ANN   \n",
       "3          2023-24   31-12-2024  HCESAFY24001ANN   \n",
       "4          2023-24   31-12-2024  HCESAFY24001ANN   \n",
       "\n",
       "                                        Download URL  \\\n",
       "0  https://esankhyiki.mospi.gov.in/datacatalogue/...   \n",
       "1  https://esankhyiki.mospi.gov.in/datacatalogue/...   \n",
       "2  https://esankhyiki.mospi.gov.in/datacatalogue/...   \n",
       "3  https://esankhyiki.mospi.gov.in/datacatalogue/...   \n",
       "4  https://esankhyiki.mospi.gov.in/datacatalogue/...   \n",
       "\n",
       "                     Data Source  \\\n",
       "0  National Sample Survey Office   \n",
       "1  National Sample Survey Office   \n",
       "2  National Sample Survey Office   \n",
       "3  National Sample Survey Office   \n",
       "4  National Sample Survey Office   \n",
       "\n",
       "                                         Description  id  \\\n",
       "0  The Household Consumption Expenditure Survey (...  28   \n",
       "1  The Household Consumption Expenditure Survey (...  28   \n",
       "2  The Household Consumption Expenditure Survey (...  28   \n",
       "3  The Household Consumption Expenditure Survey (...  28   \n",
       "4  The Household Consumption Expenditure Survey (...  28   \n",
       "\n",
       "                                         search_text  \n",
       "0  statement 1 average monthly per capita consump...  \n",
       "1  assertion 1 intermediate monthly per capita pu...  \n",
       "2  command 1 ordinary monthly per capita uptake s...  \n",
       "3  assertion 1 mean monthly per capita intake exp...  \n",
       "4  affirmation 1 mediocre monthly per capita usan...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"classification_trian_dataset.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25a4080",
   "metadata": {},
   "source": [
    "# Try Machine Learning Techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa18cb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.naive_bayes import MultinomialNB  \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "import joblib\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d07de139",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10,\n",
       " array(['ASI', 'ASUSE', 'CAMS', 'CPI', 'HCES', 'IIP', 'MIS', 'NAS', 'PLFS',\n",
       "        'WMI'], dtype=object))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ---- Step 1: Load Model & Prepare Data ----\n",
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "X = df['search_text'].tolist()\n",
    "y = df['Product']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "num_classes, le.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e2b667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 → ASI\n",
      "1 → ASUSE\n",
      "2 → CAMS\n",
      "3 → CPI\n",
      "4 → HCES\n",
      "5 → IIP\n",
      "6 → MIS\n",
      "7 → NAS\n",
      "8 → PLFS\n",
      "9 → WMI\n"
     ]
    }
   ],
   "source": [
    "for idx, label in enumerate(le.classes_):\n",
    "    print(f\"{idx} → {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abfe834",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 223/223 [00:11<00:00, 19.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LogisticRegression Training...\n",
      "Accuracy: 1.0000\n",
      "Model size: 30.98 KB\n",
      "Average inference time per batch: 1.6755 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       1.00      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n",
      "\n",
      "LinearSVC Training...\n",
      "Accuracy: 1.0000\n",
      "Model size: 30.85 KB\n",
      "Average inference time per batch: 1.3329 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       1.00      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n",
      "\n",
      "RandomForest Training...\n",
      "Accuracy: 1.0000\n",
      "Model size: 4342.70 KB\n",
      "Average inference time per batch: 30.6311 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       1.00      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n",
      "\n",
      "KNN Training...\n",
      "Accuracy: 1.0000\n",
      "Model size: 8584.83 KB\n",
      "Average inference time per batch: 38.0931 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       1.00      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n",
      "\n",
      "XGBoost Training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - iitgn.ac.in\\Desktop\\Projects\\SmartSearchify-AI-Enabled-Semantic-Search-for-eSankhyiki-Portal\\env\\lib\\site-packages\\xgboost\\training.py:183: UserWarning: [21:55:50] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9993\n",
      "Model size: 880.94 KB\n",
      "Average inference time per batch: 2.1706 ms\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      0.99      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       0.99      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n",
      "\n",
      "--- Summary ---\n",
      "                Model  Accuracy  Model Size (KB)  Avg Inference Time (ms)\n",
      "0  LogisticRegression  1.000000        30.975586                 1.675526\n",
      "1           LinearSVC  1.000000        30.846680                 1.332919\n",
      "2        RandomForest  1.000000      4342.696289                30.631145\n",
      "3                 KNN  1.000000      8584.832031                38.093090\n",
      "4             XGBoost  0.999298       880.937500                 2.170642\n"
     ]
    }
   ],
   "source": [
    "model_name = 'all-MiniLM-L6-v2'\n",
    "embedder = SentenceTransformer(model_name)\n",
    "\n",
    "X = df['search_text'].tolist()\n",
    "y = df['Product']\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y_encoded = le.fit_transform(y)\n",
    "num_classes = len(le.classes_)\n",
    "\n",
    "# Create dense embeddings (384-d vectors)\n",
    "X_embeddings = embedder.encode(X, show_progress_bar=True)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embeddings, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"LogisticRegression\": LogisticRegression(max_iter=1000),\n",
    "    \"LinearSVC\": LinearSVC(),\n",
    "    \"RandomForest\": RandomForestClassifier(n_estimators=50, n_jobs=-1),\n",
    "    \"KNN\": KNeighborsClassifier(n_neighbors=5),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric='mlogloss')\n",
    "}\n",
    "\n",
    "# Train & Evaluate\n",
    "results = []\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\n{name} Training...\")\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Save model & calculate size\n",
    "    file_path = f\"models/{name}_embedding_model.joblib\"\n",
    "    joblib.dump(model, file_path)\n",
    "    model_size_kb = os.path.getsize(file_path) / 1024  # in KB\n",
    "\n",
    "    # Inference Time (avg over batches of 512)\n",
    "    batch_size = 512\n",
    "    total_time = 0\n",
    "    num_batches = int(np.ceil(len(X_test) / batch_size))\n",
    "\n",
    "    for i in range(num_batches):\n",
    "        start_idx = i * batch_size\n",
    "        end_idx = min((i + 1) * batch_size, len(X_test))\n",
    "        X_batch = X_test[start_idx:end_idx]\n",
    "\n",
    "        start = time.time()\n",
    "        _ = model.predict(X_batch)\n",
    "        total_time += time.time() - start\n",
    "\n",
    "    avg_inference_time = (total_time * 1000 / num_batches)  # ms\n",
    "\n",
    "    preds = model.predict(X_test)\n",
    "\n",
    "    # Accuracy & Report\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    print(f\"Accuracy: {acc:.4f}\")\n",
    "    print(f\"Model size: {model_size_kb:.2f} KB\")\n",
    "    print(f\"Average inference time per batch: {avg_inference_time:.4f} ms\")\n",
    "    print(classification_report(le.inverse_transform(y_test), le.inverse_transform(preds)))\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": acc,\n",
    "        \"Model Size (KB)\": model_size_kb,\n",
    "        \"Avg Inference Time (ms)\": avg_inference_time\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(results)\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(summary_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fe7f7ae",
   "metadata": {},
   "source": [
    "# Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4d6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cf5eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 223/223 [00:11<00:00, 19.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2677\n",
      "Epoch [2/10], Loss: 0.1384\n",
      "Epoch [3/10], Loss: 0.0340\n",
      "Epoch [4/10], Loss: 0.0169\n",
      "Epoch [5/10], Loss: 0.0105\n",
      "Epoch [6/10], Loss: 0.0072\n",
      "Epoch [7/10], Loss: 0.0052\n",
      "Epoch [8/10], Loss: 0.0039\n",
      "Epoch [9/10], Loss: 0.0032\n",
      "Epoch [10/10], Loss: 0.0026\n",
      "Test Accuracy: 1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       1.00      1.00      1.00       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       1.00      1.00      1.00       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       1.00      1.00      1.00       101\n",
      "         WMI       1.00      1.00      1.00        99\n",
      "\n",
      "    accuracy                           1.00      1424\n",
      "   macro avg       1.00      1.00      1.00      1424\n",
      "weighted avg       1.00      1.00      1.00      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df['search_text'].tolist()\n",
    "y = df['Product']\n",
    "\n",
    "# Label encoding\n",
    "label_enc = LabelEncoder()\n",
    "y_encoded = label_enc.fit_transform(y)\n",
    "num_classes = len(label_enc.classes_)\n",
    "\n",
    "# Generate embeddings\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_embed = embedder.encode(X, show_progress_bar=True)  # shape: (n_samples, 384)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_dim = X_embed[0].shape[0]  \n",
    "model = TextClassifier(input_dim=input_dim, hidden_dim=256, output_dim=num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_enc.classes_))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f1aa97",
   "metadata": {},
   "source": [
    " Try of Title Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0a9e9c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 223/223 [00:04<00:00, 55.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.4749\n",
      "Epoch [2/10], Loss: 0.4624\n",
      "Epoch [3/10], Loss: 0.2013\n",
      "Epoch [4/10], Loss: 0.1214\n",
      "Epoch [5/10], Loss: 0.0848\n",
      "Epoch [6/10], Loss: 0.0627\n",
      "Epoch [7/10], Loss: 0.0481\n",
      "Epoch [8/10], Loss: 0.0381\n",
      "Epoch [9/10], Loss: 0.0316\n",
      "Epoch [10/10], Loss: 0.0273\n",
      "Test Accuracy: 0.9923\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ASI       1.00      1.00      1.00        67\n",
      "       ASUSE       1.00      1.00      1.00       115\n",
      "        CAMS       0.98      0.99      0.99       129\n",
      "         CPI       1.00      1.00      1.00        47\n",
      "        HCES       1.00      1.00      1.00       232\n",
      "         IIP       1.00      1.00      1.00       224\n",
      "         MIS       0.95      1.00      0.97       124\n",
      "         NAS       1.00      1.00      1.00       286\n",
      "        PLFS       0.99      0.96      0.97       101\n",
      "         WMI       1.00      0.95      0.97        99\n",
      "\n",
      "    accuracy                           0.99      1424\n",
      "   macro avg       0.99      0.99      0.99      1424\n",
      "weighted avg       0.99      0.99      0.99      1424\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X = df['Title'].tolist()\n",
    "y = df['Product']\n",
    "\n",
    "# Label encoding\n",
    "label_enc = LabelEncoder()\n",
    "y_encoded = label_enc.fit_transform(y)\n",
    "num_classes = len(label_enc.classes_)\n",
    "\n",
    "# Generate embeddings\n",
    "embedder = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "X_embed = embedder.encode(X, show_progress_bar=True)  # shape: (n_samples, 384)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_embed, y_encoded, test_size=0.2, random_state=42)\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_dataset = TextDataset(X_train, y_train)\n",
    "test_dataset = TextDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64)\n",
    "\n",
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "input_dim = X_embed[0].shape[0]  \n",
    "model = TextClassifier(input_dim=input_dim, hidden_dim=256, output_dim=num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/10], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "\n",
    "# Evaluation \n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs = model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.numpy())\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy_score(all_labels, all_preds):.4f}\")\n",
    "print(classification_report(all_labels, all_preds, target_names=label_enc.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c111e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
