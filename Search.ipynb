{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2191e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: bert-base-uncased\n",
      "Model Size (MB): 417.64746856689453\n",
      "Inference Time (s): 0.07618069648742676\n",
      "Test Results (Embeddings):\n",
      "Query: What are the inflation rates in India?\n",
      "Embedding: tensor([-0.1021, -0.0769], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: CPI changes over the years?\n",
      "Embedding: tensor([-0.0436, -0.1450], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: How does the CPI affect rural and urban areas?\n",
      "Embedding: tensor([-0.0610, -0.0918], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: What is the inflation rate for different states?\n",
      "Embedding: tensor([-0.0251, -0.0303], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: How is the consumer price index measured?\n",
      "Embedding: tensor([-0.0953, -0.0899], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Model: roberta-base\n",
      "Model Size (MB): 475.49121856689453\n",
      "Inference Time (s): 0.07665562629699707\n",
      "Test Results (Embeddings):\n",
      "Query: What are the inflation rates in India?\n",
      "Embedding: tensor([-0.1657,  0.2951], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: CPI changes over the years?\n",
      "Embedding: tensor([-0.1571,  0.3089], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: How does the CPI affect rural and urban areas?\n",
      "Embedding: tensor([-0.1650,  0.2969], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: What is the inflation rate for different states?\n",
      "Embedding: tensor([-0.1682,  0.2980], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Query: How is the consumer price index measured?\n",
      "Embedding: tensor([-0.1652,  0.3004], grad_fn=<SelectBackward0>)\n",
      "--------------------------------------------------\n",
      "Model: sentence-transformers/all-MiniLM-L6-v2\n",
      "Model Size (MB): 86.64404296875\n",
      "Inference Time (s): 0.0343012809753418\n",
      "Test Results (Embeddings):\n",
      "Query: What are the inflation rates in India?\n",
      "Embedding: [ 1.66111300e-03 -2.74688471e-02 -1.42535612e-01  5.24616763e-02\n",
      " -1.00523429e-02 -3.69556621e-02 -2.72521097e-02  1.56104248e-02\n",
      " -3.21224798e-03 -1.83311757e-02  7.60372579e-02 -1.24866970e-01\n",
      " -2.20087301e-02  2.72264257e-02  3.47784199e-02  2.20498648e-02\n",
      "  1.09554073e-02 -2.32934710e-02 -7.83514418e-03 -2.90299803e-02\n",
      "  1.62014738e-02 -6.24259608e-03 -4.60763425e-02 -5.51477596e-02\n",
      "  1.26321107e-01 -1.69237312e-02 -1.52379619e-02 -5.72295152e-02\n",
      " -1.15241958e-02  3.83569226e-02 -8.02071858e-03  4.79926616e-02\n",
      " -4.06995229e-02 -2.13571154e-02 -9.42161027e-03 -2.30628178e-02\n",
      "  3.61630134e-02  6.17649071e-02  5.62088676e-02 -2.40208786e-02\n",
      " -1.39635545e-03 -5.44775762e-02  3.17069814e-02 -5.69151603e-02\n",
      "  3.44232097e-02  1.40101323e-02 -4.06105816e-03  1.09051786e-01\n",
      "  9.65262647e-04  6.94374591e-02 -2.82506295e-03  5.55854710e-03\n",
      " -1.53322695e-02  2.67526675e-02  3.25867012e-02 -2.67565604e-02\n",
      " -1.93003397e-02 -5.14403433e-02  3.60929109e-02 -9.35268402e-03\n",
      " -7.32598454e-02  2.89761256e-02 -6.26821741e-02  3.58091295e-02\n",
      "  5.22514880e-02 -2.68068425e-02  1.63836237e-02 -4.16324027e-02\n",
      " -2.75930222e-02  3.28209624e-02 -5.49548119e-03 -9.96826123e-03\n",
      " -2.62731649e-02 -8.32137764e-02 -6.55508861e-02 -5.92955984e-02\n",
      "  1.83616616e-02  2.05059126e-02  6.28560921e-03 -2.08757408e-02\n",
      "  3.88720147e-02 -3.02334335e-02  5.35402596e-02 -3.44258472e-02\n",
      " -1.93023682e-02 -5.03481776e-02  1.02756225e-01  2.21417565e-03\n",
      " -2.17710938e-02  2.43477281e-02  1.10594600e-01  7.90603459e-02\n",
      " -7.03834072e-02  1.84799340e-02 -3.00250426e-02  3.67907360e-02\n",
      " -3.54245864e-02 -1.38730602e-02 -1.45607721e-02  3.90083678e-02\n",
      " -4.88778111e-03 -4.92878146e-02  1.83988623e-02  8.37157890e-02\n",
      " -1.20060809e-01 -5.97646907e-02 -3.83041380e-03  2.44046357e-02\n",
      "  3.63886505e-02  2.85401344e-02 -9.00703371e-02 -1.35814371e-02\n",
      " -1.90539639e-02 -6.74261525e-02 -4.82247472e-02 -5.62044568e-02\n",
      "  3.11770476e-02 -3.12611647e-02  6.43375367e-02  6.15301505e-02\n",
      "  1.10710822e-02  7.28571191e-02  2.05744933e-02  1.27859497e-02\n",
      " -5.82147986e-02 -1.12222238e-02 -8.96435380e-02 -3.95602559e-33\n",
      "  7.99563807e-03  2.71262979e-04  6.41932851e-03 -9.59464088e-02\n",
      " -5.72959073e-02 -7.64781889e-03 -1.40064592e-02  2.10718960e-02\n",
      "  9.13532227e-02 -4.10744138e-02 -2.70990506e-02 -1.90666914e-02\n",
      " -1.81560905e-03 -5.96360825e-02  9.48335752e-02 -7.67939687e-02\n",
      " -3.85378897e-02  6.46633506e-02  6.50783107e-02  3.02116796e-02\n",
      " -3.12335324e-02 -2.03920137e-02  1.83393396e-02  3.04169115e-02\n",
      " -3.77631299e-02  9.53254849e-03  6.60193488e-02  9.79580265e-03\n",
      "  4.50269096e-02  2.39949096e-02  9.18308273e-02  5.71236275e-02\n",
      "  2.32963711e-02 -6.08112626e-02 -6.59482405e-02  5.40421531e-02\n",
      "  2.71720551e-02  2.48293765e-02 -2.54044011e-02 -7.74097908e-03\n",
      " -8.67342483e-03 -7.09426869e-03  1.41666746e-02  1.54130347e-02\n",
      " -3.80738005e-02  5.07887304e-02  6.96581155e-02 -2.46624481e-02\n",
      " -8.68668593e-03 -4.73982319e-02 -4.99019735e-02  1.79187544e-02\n",
      " -1.24807812e-01  3.66085267e-04  5.09302430e-02  4.37570661e-02\n",
      " -5.55790141e-02 -5.91279976e-02  6.54153526e-03  6.41829669e-02\n",
      " -2.01514699e-02  6.83160871e-03 -4.40899469e-02 -2.98775658e-02\n",
      " -9.24877450e-02 -2.35255156e-02  8.13885182e-02 -2.33472772e-02\n",
      " -5.15650883e-02  4.12954576e-02 -3.43280705e-03 -1.01622310e-03\n",
      "  1.50481705e-02  1.41572831e-02 -6.64782301e-02  8.38065296e-02\n",
      "  5.83140440e-02 -8.58857185e-02 -5.15547730e-02 -1.15715312e-02\n",
      " -7.96908364e-02 -8.70802347e-03  5.68059534e-02 -5.71083985e-02\n",
      "  6.63941875e-02 -5.15564792e-02 -5.68065867e-02  1.58893541e-02\n",
      "  6.48430884e-02 -5.16432784e-02 -3.75961028e-02 -4.01559938e-03\n",
      " -1.24618253e-02 -6.64447546e-02  2.91790962e-02  1.33516092e-33\n",
      "  3.79089639e-02  5.87460510e-02 -5.39401360e-02  1.28621489e-01\n",
      "  2.22227480e-02 -8.18592776e-03  1.08112991e-02  7.46632293e-02\n",
      "  1.06986977e-01 -1.34412609e-02 -8.05400833e-02  7.22764730e-02\n",
      "  3.76849174e-02  6.22104146e-02  3.32529023e-02 -2.92056743e-02\n",
      "  2.52816994e-02  3.73626687e-02 -3.21220942e-02 -4.94315065e-02\n",
      " -7.77905062e-02  3.69436555e-02 -1.40068084e-02  6.00455068e-02\n",
      " -6.61234781e-02 -7.17708236e-03 -1.05258085e-01 -4.68597375e-02\n",
      " -2.07542311e-02  9.22289491e-03  3.66112962e-02 -3.57619449e-02\n",
      "  1.46454293e-02  5.93943745e-02 -6.92059472e-02 -4.26121876e-02\n",
      "  1.49705615e-02 -2.37255618e-02  2.60602050e-02  3.42759527e-02\n",
      "  3.18701454e-02  8.04041177e-02  3.96416616e-03  2.19195355e-02\n",
      "  3.58414948e-02 -3.68790030e-02  6.22539893e-02  1.66501626e-02\n",
      "  4.48720604e-02 -8.68309364e-02  1.82912927e-02 -2.79390737e-02\n",
      "  5.80912828e-03 -3.25349858e-04 -6.21244982e-02 -4.75554690e-02\n",
      "  6.75334875e-03  9.22149699e-03 -2.44850223e-03 -7.75449276e-02\n",
      "  6.95821643e-02  3.72623019e-02  2.87346616e-02  7.45643005e-02\n",
      "  4.47202027e-02  7.54413777e-04  8.80086422e-02 -3.50073911e-02\n",
      "  5.44170514e-02 -2.30113510e-02  1.00433409e-01 -7.45470673e-02\n",
      " -3.81748863e-02  4.25866209e-02 -5.02328351e-02  6.85446188e-02\n",
      "  8.35028887e-02  5.43335304e-02  7.40734413e-02 -6.66803960e-03\n",
      "  2.80809333e-03  5.54371066e-02  2.14521829e-02 -1.18855111e-01\n",
      " -9.40887257e-02 -6.56774864e-02  8.70824233e-02 -3.91240306e-02\n",
      " -1.22288093e-02 -2.06903205e-03 -1.06334217e-01  1.13649555e-02\n",
      "  4.33114879e-02 -2.34061573e-02  5.81275020e-03 -1.65114944e-08\n",
      " -1.76526643e-02 -7.85778910e-02 -9.78378020e-03  4.83785048e-02\n",
      "  3.25420499e-02  6.71467856e-02  5.34866303e-02  6.55898973e-02\n",
      "  4.35836278e-02 -3.02433390e-02  6.44124970e-02 -3.33272777e-02\n",
      " -2.21208446e-02 -4.26607542e-02  1.94300655e-02 -3.09156086e-02\n",
      " -5.15705422e-02  1.12295590e-01 -6.07034052e-03 -1.07755721e-01\n",
      "  1.07363230e-02  1.44242406e-01  6.53015897e-02 -8.56861174e-02\n",
      " -2.34073717e-02  3.79281975e-02  6.08824976e-02  7.41858557e-02\n",
      "  3.95316677e-03  5.15738614e-02 -7.96320513e-02  2.55865585e-02\n",
      "  1.77691709e-02 -1.51691452e-01 -4.15549167e-02  7.96190649e-02\n",
      "  6.35041818e-02  1.39787123e-02  6.11802228e-02 -1.21405488e-02\n",
      "  1.25162052e-02 -2.55579595e-02 -7.21622780e-02 -7.00844601e-02\n",
      "  4.53413762e-02 -2.13128398e-03 -4.23021503e-02 -1.32333813e-02\n",
      " -5.54355979e-02 -8.18451867e-02 -8.47329106e-03  8.31610113e-02\n",
      " -1.95347471e-03  2.28399485e-02 -3.60976756e-02 -3.77724953e-02\n",
      " -9.60619822e-02 -1.56553760e-02 -1.00675978e-01 -5.00682683e-04\n",
      "  6.43210858e-02 -4.93381433e-02  3.82427275e-02  3.34448516e-02]\n",
      "--------------------------------------------------\n",
      "Query: CPI changes over the years?\n",
      "Embedding: [-5.51770218e-02 -1.33632785e-02 -1.24474280e-02  3.44084725e-02\n",
      "  1.12290330e-01 -3.08040753e-02 -7.92349428e-02 -1.54318484e-02\n",
      " -8.16507712e-02 -2.65868809e-02  1.11254387e-01  6.18363991e-02\n",
      " -5.71589358e-02  1.37765873e-02 -5.59727624e-02 -4.95207570e-02\n",
      " -3.13693359e-02  2.90764426e-03  4.09280658e-02 -1.65400580e-02\n",
      " -5.16011156e-02 -5.18884622e-02 -1.20222270e-02 -3.25973630e-02\n",
      "  5.58838844e-02  9.51796211e-03 -4.44334559e-02 -2.30427203e-03\n",
      "  9.60326660e-03  3.02341953e-03 -6.67576790e-02  3.98607887e-02\n",
      "  2.08392125e-02 -3.43672559e-02  3.52200717e-02 -1.07547948e-02\n",
      "  9.35739055e-02  5.92211112e-02 -2.02922393e-02  2.11127438e-02\n",
      "  1.63520537e-02 -1.05675571e-01  1.81219564e-03 -4.30513546e-02\n",
      " -2.48238519e-02  2.65375376e-02  5.02768978e-02  7.23044872e-02\n",
      " -5.64357042e-02  8.84273425e-02 -1.18984692e-02  2.61412282e-02\n",
      " -1.10268192e-02  1.87682081e-02  3.66894901e-02 -1.33044496e-02\n",
      "  2.64026579e-02  3.71542983e-02  5.67633333e-03 -3.55816409e-02\n",
      " -9.00180712e-02  5.00794090e-02 -8.50912631e-02  1.58131775e-02\n",
      "  8.64569470e-02 -2.57937214e-03  6.81046024e-03 -6.41000718e-02\n",
      " -7.01262790e-04  3.61394533e-03  1.34477690e-02 -2.85978336e-02\n",
      " -1.36262877e-02 -8.46457705e-02 -1.19082192e-02 -7.92673752e-02\n",
      " -6.22800225e-03  3.50623392e-02  5.32388836e-02 -2.11464595e-02\n",
      "  7.81508908e-02 -1.22470409e-02 -2.33712443e-03 -4.29780483e-02\n",
      " -4.18177806e-02 -4.92359921e-02  2.35089343e-02 -7.34436065e-02\n",
      " -4.98530045e-02 -4.96168099e-02  1.03784107e-01  5.91143556e-02\n",
      "  4.45122197e-02  4.95110713e-02 -7.13595701e-03  3.09908539e-02\n",
      " -7.12758750e-02  5.13027757e-02  8.54523331e-02  6.57644644e-02\n",
      " -1.47979511e-02  1.54391034e-02 -5.03234342e-02  5.80992438e-02\n",
      " -7.30686486e-02 -1.71190239e-02 -2.18812395e-02  9.01386142e-03\n",
      " -5.14991209e-02  4.17950936e-02 -4.44625579e-02 -4.33861203e-02\n",
      " -3.86133157e-02 -2.08943989e-02  4.23928909e-02 -1.02834046e-01\n",
      "  2.24878285e-02  1.77212444e-03 -4.75317687e-02  1.11181013e-01\n",
      "  9.56513435e-02  4.69706841e-02 -8.90010297e-02  2.55728960e-02\n",
      " -4.21444178e-02  4.09384668e-02  1.89538114e-02 -4.44323493e-33\n",
      "  2.71174368e-02  9.88896005e-03  1.34040862e-02  1.12802330e-02\n",
      " -8.28782171e-02 -3.01318336e-02 -3.31660882e-02  2.71138828e-02\n",
      "  9.41391215e-02 -2.36575454e-02  1.12451185e-02  9.17917639e-02\n",
      " -4.27694060e-02 -2.75815348e-03  7.92605728e-02 -7.17877373e-02\n",
      " -6.64054900e-02  7.81268254e-02  1.07877821e-01  2.21618824e-02\n",
      "  6.25475263e-03  4.06824537e-02  6.47786781e-02 -1.41611472e-02\n",
      "  6.47870749e-02  3.78386937e-02 -4.81859408e-02  8.97886511e-03\n",
      "  3.31704356e-02 -9.56737157e-03  1.18871324e-01  2.03827173e-02\n",
      " -5.53963780e-02 -6.49823472e-02 -8.05816948e-02  3.88365053e-02\n",
      "  2.20663399e-02 -2.82615097e-03 -3.58540043e-02 -5.59226312e-02\n",
      "  9.40964743e-03  9.97865666e-03 -4.07723570e-03  4.13597375e-02\n",
      "  4.01854701e-02  5.82842082e-02  5.35126999e-02 -8.23641494e-02\n",
      " -3.29939649e-02 -3.76925208e-02  1.05141271e-02 -1.85660217e-02\n",
      " -6.32977709e-02 -2.34875455e-02 -1.73358992e-02  1.10520720e-02\n",
      " -2.62938831e-02 -6.50627762e-02  3.20428871e-02 -5.02240136e-02\n",
      "  6.14591911e-02  6.88833296e-02 -1.95789430e-02  3.54607180e-02\n",
      "  9.84646380e-03  9.90213007e-02  2.72484124e-02  5.94413988e-02\n",
      " -3.44052874e-02  8.51486027e-02 -2.88496166e-02 -2.14907210e-02\n",
      " -3.45431529e-02  4.34108004e-02 -2.10880991e-02  6.17988221e-02\n",
      " -6.41283467e-02 -4.09351550e-02 -4.31279987e-02 -1.10471763e-01\n",
      " -1.24818683e-01  2.24951450e-02  6.63830936e-02 -8.82996619e-02\n",
      "  9.02670994e-02 -3.43358107e-02  1.23350946e-02  5.47906831e-02\n",
      "  1.29951224e-01 -5.18636368e-02 -3.48691829e-02 -2.80654021e-02\n",
      " -2.56011467e-02  1.30393226e-02 -5.53820049e-03  1.63999351e-33\n",
      " -5.52817322e-02  2.03537997e-02 -5.50570749e-02  6.16848804e-02\n",
      " -6.38988614e-02 -3.68843153e-02 -1.54738373e-03  3.28021683e-02\n",
      "  1.13045759e-01  3.69837955e-02 -1.98997110e-02 -4.84384261e-02\n",
      "  3.09444349e-02  6.03340864e-02  2.73219049e-02  7.97489192e-03\n",
      " -2.81796027e-02 -3.05634700e-02 -3.01326551e-02 -3.79834250e-02\n",
      " -1.06154986e-01  4.73186523e-02  7.54537201e-03  4.65992130e-02\n",
      " -1.37697160e-02  1.13899924e-03 -4.48722765e-02 -7.97163099e-02\n",
      "  7.74376914e-02 -7.61076584e-02 -4.42766398e-02 -3.78234982e-02\n",
      "  1.92213710e-02  6.16299286e-02 -4.88617495e-02  1.91865098e-02\n",
      " -2.36981008e-02 -7.43818134e-02 -3.89194419e-03  1.15710879e-02\n",
      " -2.48680096e-02 -3.92622361e-03  3.07276417e-02  4.62601446e-02\n",
      "  2.98891664e-02  7.47575611e-03 -4.38989550e-02  5.07891588e-02\n",
      "  5.09857163e-02 -1.09496258e-01  2.60413326e-02  4.17825803e-02\n",
      "  1.25900824e-02 -5.34346700e-03 -6.77897334e-02  4.87871282e-02\n",
      " -1.31353661e-02  3.85992453e-02 -4.15528193e-02 -1.83901135e-02\n",
      "  4.57393229e-02  1.77854032e-03 -7.14838132e-02  5.48861036e-03\n",
      "  5.15647493e-02  1.83697939e-02  4.00823019e-02 -1.78530991e-01\n",
      "  8.29793960e-02 -3.95582709e-03  1.50887728e-01 -5.26719727e-02\n",
      " -6.74436465e-02 -2.85248645e-02 -4.10950221e-02  5.69909113e-03\n",
      "  3.17851491e-02  9.87303257e-02 -2.66791824e-02 -1.76123697e-02\n",
      " -6.97430074e-02 -3.25927958e-02 -7.71150179e-03 -6.42883703e-02\n",
      " -3.38930562e-02 -2.36956123e-03  5.81059679e-02 -6.79568723e-02\n",
      " -6.29561469e-02  5.07409982e-02 -4.20640744e-02 -1.69809442e-02\n",
      " -6.71264622e-03 -2.48402227e-02 -2.15140097e-02 -1.47631640e-08\n",
      "  5.03834561e-02  6.78772703e-02 -9.31983348e-03  8.42856020e-02\n",
      "  4.97175455e-02 -2.61428840e-02  2.74890452e-03  4.43107039e-02\n",
      "  3.22531760e-02  1.85631476e-02  5.90019375e-02  4.04628217e-02\n",
      "  1.04071639e-01 -4.38600816e-02 -1.39145404e-02 -3.51765216e-03\n",
      " -8.82065669e-02  2.76295468e-02 -2.13020598e-04 -3.05683259e-02\n",
      "  2.43066195e-02  1.25102341e-01 -5.00062760e-03  2.99181137e-02\n",
      " -2.08454281e-02  3.57859842e-02  5.08919358e-02  2.22768672e-02\n",
      " -3.40858847e-03  4.57659177e-02 -2.29119044e-02  7.05030834e-05\n",
      "  7.58588314e-02 -1.02493025e-01  3.32425609e-02 -3.76064982e-03\n",
      "  4.51065563e-02  7.14290515e-03  5.12825362e-02 -6.91331625e-02\n",
      "  1.83427669e-02 -3.36192697e-02 -1.00071490e-01 -7.13439425e-03\n",
      "  2.63780672e-02 -1.48139810e-02 -6.17012382e-02 -2.32951678e-02\n",
      "  9.73419659e-03 -8.28634724e-02  1.56918373e-02  5.50044589e-02\n",
      " -3.46496254e-02  9.37688444e-03  5.57525866e-02 -6.84531927e-02\n",
      " -3.02221067e-02  3.55213124e-04 -3.74932848e-02 -4.81084846e-02\n",
      "  7.20756203e-02 -5.94941005e-02  1.40493847e-02  4.80224043e-02]\n",
      "--------------------------------------------------\n",
      "Query: How does the CPI affect rural and urban areas?\n",
      "Embedding: [ 4.63116579e-02 -1.79604616e-03  1.63441598e-02  6.54657483e-02\n",
      "  1.01555772e-01 -6.87241880e-03 -6.51166663e-02 -3.01848967e-02\n",
      " -7.08757266e-02 -3.78012173e-02  1.33968890e-01  7.29069952e-03\n",
      " -2.59945225e-02  2.36970130e-02 -2.82421261e-02 -1.16529223e-02\n",
      "  4.08775657e-02  2.17348766e-02  3.26424167e-02 -3.48041840e-02\n",
      " -2.36069318e-02 -1.76893044e-02  2.80173477e-02 -1.22896824e-02\n",
      "  2.48392764e-02 -1.65690705e-02 -1.37954764e-02  1.39374975e-02\n",
      " -2.17781123e-02  8.02933425e-03 -4.22919728e-02  1.01688161e-01\n",
      "  3.51306908e-02 -3.47674415e-02  1.83318574e-02  3.83010097e-02\n",
      "  4.98867445e-02  7.77315944e-02  3.87025774e-02 -1.51145216e-02\n",
      "  5.29103912e-02 -1.28211051e-01  3.39278020e-02 -9.36190411e-02\n",
      "  3.93918455e-02 -3.76776718e-02  4.08873670e-02  6.13223203e-02\n",
      " -4.74205464e-02 -5.20384579e-04  2.47170143e-02  3.57865356e-02\n",
      " -1.27336616e-02  5.42208515e-02 -1.03679439e-02 -3.44309621e-02\n",
      "  5.61224949e-03  3.19122970e-02  6.62822602e-03  2.88325623e-02\n",
      " -6.56784475e-02  2.46785302e-02 -7.82227367e-02  2.39111986e-02\n",
      "  1.47915751e-01 -1.84337795e-02 -2.58769803e-02 -6.10284545e-02\n",
      " -3.20482962e-02 -6.85154945e-02  7.89313838e-02 -9.31421071e-02\n",
      " -3.28803770e-02 -4.73255254e-02 -1.40449042e-02 -5.54195829e-02\n",
      " -8.55425373e-02  5.53923845e-02  4.22092825e-02 -5.04182056e-02\n",
      "  6.50818348e-02 -2.58743856e-03  2.29900647e-02 -4.22573462e-02\n",
      " -5.69556281e-02 -4.16216515e-02  4.26043682e-02 -5.50137125e-02\n",
      " -4.66926359e-02 -2.38090791e-02  8.95022973e-02  7.14820400e-02\n",
      " -3.40053104e-02  2.32320745e-02  3.69864074e-03  9.33784805e-03\n",
      " -8.40243325e-02  1.57563034e-02 -4.06472832e-02  1.42645817e-02\n",
      " -3.16624306e-02 -1.90768335e-02 -8.95820782e-02  3.61970775e-02\n",
      " -5.97069152e-02 -5.97970895e-02 -2.58247796e-02 -2.59641819e-02\n",
      " -5.19316941e-02  6.31313920e-02 -2.84838770e-02  1.25128438e-03\n",
      " -6.88941032e-02  1.27721550e-02  2.95960344e-02 -7.44882226e-02\n",
      "  1.16649911e-01  1.00978343e-02 -4.29417714e-02  4.77977507e-02\n",
      " -1.03331013e-02 -3.38410772e-02 -1.07970521e-01  1.40069965e-02\n",
      "  4.73521762e-02  2.83177439e-02 -3.73559035e-02 -4.44187944e-33\n",
      "  4.46886383e-03  1.39942616e-02  4.78957925e-04  1.97495595e-02\n",
      " -4.81963754e-02 -4.88765277e-02 -6.17897976e-03 -3.47148702e-02\n",
      "  8.82752314e-02 -1.19840484e-02  2.47246493e-02  6.36862293e-02\n",
      "  5.10063693e-02  3.50777581e-02  7.81898052e-02 -2.03209538e-02\n",
      " -4.39586006e-02  5.05713373e-02  5.15150465e-02  5.19305393e-02\n",
      " -1.47108557e-02  3.82952392e-02 -1.82000734e-02  4.60120430e-03\n",
      "  1.52790239e-02  2.63610464e-02 -3.30274068e-02  3.36873680e-02\n",
      "  4.80984487e-02  2.81437989e-02  6.15514889e-02  3.95476669e-02\n",
      " -7.72692915e-03 -6.89483285e-02 -6.60532787e-02  8.09155405e-02\n",
      " -4.39930372e-02  2.39724871e-02 -9.95265841e-02  1.81311052e-02\n",
      " -5.58424443e-02  2.56049157e-06 -2.10790262e-02  2.12617125e-02\n",
      "  6.06765971e-02  2.98094731e-02  6.40508756e-02 -6.69918656e-02\n",
      " -3.90757434e-02 -1.47933308e-02 -8.86195991e-03 -7.70783424e-03\n",
      " -5.17672673e-02  4.58174711e-03  2.63091419e-02  3.51109579e-02\n",
      " -6.16657408e-03 -9.82798189e-02  1.86493732e-02  1.19212996e-02\n",
      "  3.24104726e-02  1.06615983e-02 -1.65877584e-02 -4.43607103e-03\n",
      "  5.99849485e-02 -3.49572487e-02  5.50553240e-02  5.24081104e-02\n",
      " -6.33015065e-03  5.25223352e-02  4.23344448e-02 -1.91188529e-02\n",
      "  2.28895284e-02  9.45032090e-02 -2.99388673e-02  4.50460725e-02\n",
      " -1.01907626e-01  9.95360687e-03 -3.15056965e-02 -5.19335009e-02\n",
      " -9.50033367e-02 -4.96714264e-02  1.24136759e-02 -6.07483238e-02\n",
      "  9.59905833e-02 -3.57974283e-02 -1.36703695e-03  3.15891616e-02\n",
      "  8.48288387e-02 -9.37276036e-02 -5.67791872e-02  3.59380320e-02\n",
      " -7.43812695e-02  2.41321512e-02  4.93895635e-03  1.37622122e-33\n",
      " -5.79273775e-02  3.96378674e-02 -6.81654811e-02  3.02021783e-02\n",
      " -7.57980570e-02 -5.53644821e-02  5.41192666e-03 -3.61321643e-02\n",
      "  1.69696629e-01  5.95413111e-02 -1.44964501e-01 -1.36614097e-02\n",
      "  3.40195857e-02  5.81931174e-02  8.32654685e-02 -1.09418044e-02\n",
      "  5.07297553e-03  3.34002189e-02  1.60606112e-02 -5.01774158e-03\n",
      " -9.39883441e-02  1.24495095e-02  6.01106649e-03  5.24096750e-02\n",
      " -2.45307013e-02  2.85017919e-02 -1.65445298e-01 -6.76919445e-02\n",
      "  2.14793403e-02 -6.25959560e-02 -2.02351306e-02 -2.47160047e-02\n",
      " -1.71766654e-02  5.78348860e-02 -7.83160999e-02  2.25374866e-02\n",
      " -5.40345348e-03 -5.25514334e-02 -3.96242877e-03 -2.78145391e-02\n",
      " -1.04902377e-02 -1.27444277e-02  2.99277902e-02  6.60522515e-03\n",
      " -7.14251597e-04 -5.48561476e-02  9.01680347e-03  3.95449288e-02\n",
      " -4.54198420e-02 -4.76721898e-02  7.00527504e-02  1.00241877e-01\n",
      " -8.24799761e-04  5.20144589e-02 -4.41397689e-02  1.34683196e-02\n",
      "  6.82281852e-02  2.56427731e-02 -9.32334885e-02  2.55776625e-02\n",
      "  7.43599012e-02  4.08649258e-02 -7.96998143e-02  3.18351649e-02\n",
      "  3.88458036e-02 -1.53726172e-02  3.93097699e-02 -1.23733848e-01\n",
      "  1.01018749e-01  6.14614575e-04  8.70138779e-02 -3.91341560e-02\n",
      " -3.33305374e-02 -3.64181660e-02 -8.38448256e-02  5.09003103e-02\n",
      "  6.35857284e-02  1.71307549e-01  1.17075201e-02  2.45245430e-03\n",
      " -2.37321425e-02 -4.61196415e-02 -5.26076593e-02 -1.12357445e-01\n",
      " -3.07836551e-02 -2.34415494e-02  4.20624875e-02 -1.06037661e-01\n",
      " -2.01594438e-02  6.16063811e-02 -1.12695254e-01  1.56156695e-03\n",
      " -9.94440168e-03 -3.98488678e-02 -1.58366803e-02 -1.77614474e-08\n",
      "  5.41264638e-02  3.34056988e-02 -1.27945673e-02  4.41879928e-02\n",
      "  3.85319367e-02  1.18106036e-02  1.07571564e-03  1.04200721e-01\n",
      " -3.40518029e-03  1.00226261e-01  1.87821332e-02  5.08286729e-02\n",
      "  3.98607664e-02 -1.59839485e-02  1.19578354e-02 -2.18069348e-02\n",
      "  9.69985593e-03  4.49302569e-02 -5.34788258e-02 -1.38988823e-03\n",
      "  7.62104839e-02  4.24037017e-02 -7.17117861e-02 -5.55946119e-03\n",
      "  1.02217114e-02 -1.50950777e-03  6.71290606e-02  1.27275838e-02\n",
      "  2.26823166e-02  7.71519020e-02 -2.14713663e-02 -1.61441695e-02\n",
      " -1.45682087e-03 -1.73269194e-02  1.09892907e-02  4.52680932e-03\n",
      "  5.91459051e-02  1.74925067e-02  4.04815786e-02 -5.23093715e-02\n",
      "  6.05999632e-03 -5.50855324e-02 -4.53366898e-02 -3.50109749e-02\n",
      "  6.94331601e-02 -7.71489292e-02 -5.40065952e-02  1.48518558e-03\n",
      "  2.17420105e-02  2.59655132e-03 -4.79401127e-02 -5.13834646e-04\n",
      " -6.03970475e-02  2.64350921e-02  4.21538875e-02 -9.83042642e-02\n",
      "  4.59247315e-03 -2.33305246e-02  2.13853847e-02 -3.55503261e-02\n",
      "  3.73670063e-03 -1.17769197e-03  1.46436319e-02  4.22194786e-03]\n",
      "--------------------------------------------------\n",
      "Query: What is the inflation rate for different states?\n",
      "Embedding: [ 3.77697684e-02 -2.89947204e-02 -9.13279057e-02  7.67927021e-02\n",
      " -2.55728234e-02 -2.50352770e-02 -8.04565772e-02 -3.97265069e-02\n",
      " -1.36058237e-02 -2.01725941e-02  5.37378155e-02 -8.01710412e-02\n",
      "  1.26299970e-02  3.99941690e-02 -1.76896434e-02  2.16297042e-02\n",
      " -1.85544081e-02 -7.54382238e-02 -1.79912597e-02  1.66428462e-02\n",
      "  7.77630210e-02 -4.61523049e-02 -7.49180913e-02 -3.92784476e-02\n",
      "  1.63384303e-01  1.49022657e-02 -2.39186399e-02 -4.36337069e-02\n",
      "  7.17151258e-03  4.19938155e-02 -5.34927882e-02 -2.46219933e-02\n",
      " -4.34941612e-02 -4.94299270e-02  3.47640850e-02 -4.49199937e-02\n",
      "  7.13225529e-02  5.09759970e-02 -3.08578346e-05  2.62195859e-02\n",
      " -3.87266581e-03  1.12327907e-04  3.46510969e-02 -2.37239823e-02\n",
      "  9.44357482e-04  4.52967770e-02  2.14192830e-03  1.38797671e-01\n",
      "  1.99067174e-03  8.06977823e-02  3.82433459e-02  3.56802754e-02\n",
      " -1.00160735e-02  4.80566658e-02  4.71941940e-02  3.97847034e-02\n",
      "  1.41194770e-02  7.60462554e-03 -2.77199633e-02 -5.99712599e-04\n",
      " -9.80480537e-02  1.22620249e-02 -3.99432182e-02  1.26093114e-02\n",
      "  8.65414739e-02  3.84129286e-02 -2.69780383e-02 -6.10686839e-02\n",
      " -1.11879848e-01 -4.15140912e-02 -2.72429036e-03 -1.58328824e-02\n",
      " -1.71566829e-02 -6.01065196e-02 -1.84620544e-02 -5.70586063e-02\n",
      "  3.88687961e-02 -6.42633438e-03  3.72301452e-02 -2.76983017e-03\n",
      "  6.83186874e-02 -4.58174907e-02  3.60529963e-03 -9.90381092e-02\n",
      " -1.74405016e-02 -8.43265876e-02  6.21241145e-02 -4.98636486e-03\n",
      "  2.90443804e-02  2.28129774e-02  6.51022345e-02  4.57925051e-02\n",
      " -5.60076386e-02 -4.59613241e-02 -3.16739380e-02  7.66852126e-02\n",
      " -1.36636095e-02  2.64252368e-02  2.99075544e-02  1.49478093e-02\n",
      "  4.45553362e-02 -7.88101405e-02  7.29278699e-02  7.97976777e-02\n",
      " -5.64784631e-02  8.99564289e-03 -3.23387757e-02  4.58348803e-02\n",
      " -1.08327158e-02  8.79552811e-02 -3.27558964e-02 -1.24998111e-02\n",
      "  2.31024306e-02 -3.75208557e-02 -4.06617038e-02 -2.89120041e-02\n",
      "  5.08776717e-02 -7.71592185e-02  8.03444311e-02  2.35555917e-02\n",
      " -8.97860434e-03  7.12558976e-04  1.42437126e-02  3.53165381e-02\n",
      " -2.63869353e-02  2.47971229e-02 -1.06709346e-01 -3.93190518e-33\n",
      "  1.06930351e-02 -6.22536503e-02  4.46921512e-02 -5.80156036e-02\n",
      " -6.73940107e-02  6.78952225e-03  7.84258544e-03 -2.75231935e-02\n",
      "  4.87500392e-02 -6.28008470e-02 -2.00155238e-03  1.77042894e-02\n",
      "  3.53141129e-02 -3.99896083e-03  6.79852888e-02 -8.55869949e-02\n",
      " -3.05401217e-02  6.11285158e-02  3.82946990e-02  5.33426227e-03\n",
      " -2.59960238e-02 -5.00319246e-03 -4.69927713e-02 -1.45049756e-02\n",
      " -8.11922625e-02  7.19573861e-03 -1.57802587e-03  3.31948735e-02\n",
      " -3.67233786e-03 -1.28668295e-02  7.29677901e-02  5.41063696e-02\n",
      "  6.29532635e-02 -3.80666852e-02 -3.72894220e-02  4.61592004e-02\n",
      "  5.73090687e-02  2.00312603e-02 -1.51442913e-02 -5.82357161e-02\n",
      "  2.78336946e-02 -1.30610047e-02  2.48472616e-02  6.63346052e-02\n",
      " -2.38508470e-02  4.26745005e-02  4.05946039e-02 -8.08151215e-02\n",
      " -3.40835825e-02 -6.60360530e-02 -3.25268023e-02  1.40868109e-02\n",
      " -1.21863194e-01 -3.22507434e-02  5.37458174e-02  2.46804822e-02\n",
      " -6.40714169e-02 -4.58110087e-02  3.13723981e-02  3.48628238e-02\n",
      " -4.14266922e-02  5.05947955e-02 -8.65539722e-03 -1.20667135e-02\n",
      " -4.22849655e-02 -3.78462449e-02  5.95423095e-02  6.78850487e-02\n",
      " -5.24391495e-02  7.20887855e-02  3.95675898e-02  2.38578357e-02\n",
      "  2.62274854e-02  1.38061568e-02  1.83469104e-03  3.95025350e-02\n",
      "  1.04709789e-01 -4.58572917e-02 -3.47673558e-02 -2.82247718e-02\n",
      " -1.06037796e-01 -6.64589703e-02  9.51015297e-03  2.60296278e-03\n",
      "  2.23018881e-02 -6.85478970e-02 -8.23773295e-02  4.07379828e-02\n",
      "  5.08856438e-02 -3.32755409e-02 -6.92824796e-02  4.41125175e-03\n",
      "  2.47784127e-02 -3.61608826e-02  2.23442297e-02  1.50747076e-33\n",
      "  2.94412263e-02  2.49726027e-02  1.67388078e-02  1.28932908e-01\n",
      "  1.98643003e-02 -3.82403173e-02  4.77775522e-02  6.76589906e-02\n",
      "  6.73557296e-02 -6.57921657e-02 -1.07904628e-01  4.50252965e-02\n",
      "  7.47788623e-02  6.84480816e-02  5.04201502e-02  1.12451389e-02\n",
      "  3.15853059e-02  3.85434143e-02  9.75183025e-03 -2.79602073e-02\n",
      " -9.34437588e-02 -3.02344672e-02 -5.41571043e-02  9.33594555e-02\n",
      " -5.85885569e-02 -4.30254154e-02 -1.01297468e-01 -7.67155811e-02\n",
      " -1.92394741e-02 -9.40882601e-04  1.03952189e-03 -1.25520667e-02\n",
      "  1.66072119e-02  9.86810178e-02 -1.11061536e-01 -3.59205268e-02\n",
      " -2.15708204e-02 -5.53508438e-02  1.50644127e-02  4.07699961e-03\n",
      "  4.93786260e-02  4.30532452e-03 -1.16565377e-02  4.59317304e-02\n",
      "  2.71266662e-02  4.29334603e-02  4.28273678e-02  2.92704720e-03\n",
      "  2.04389766e-02 -5.30915856e-02 -6.27564862e-02 -3.86888580e-03\n",
      " -3.93224731e-02  6.36755154e-02 -9.45488587e-02 -3.82308029e-02\n",
      "  5.70671458e-04  4.77615707e-02 -2.82838531e-02 -5.26326820e-02\n",
      "  4.39971238e-02  1.35607831e-02  2.37626135e-02  2.08364949e-02\n",
      "  5.96232191e-02  5.38072689e-03  7.94066787e-02 -2.77263913e-02\n",
      "  6.73598424e-02  1.86551269e-02  9.14324373e-02 -2.10962463e-02\n",
      " -8.09508655e-03  3.91211454e-03 -2.25913152e-02  5.61198220e-02\n",
      "  1.25975177e-01  6.10710308e-02  2.57009398e-02  3.75702372e-03\n",
      " -4.35710512e-02  1.51703050e-02 -3.88967991e-02 -6.01045042e-02\n",
      " -1.07571475e-01 -8.08420032e-03  6.24765977e-02 -6.33439869e-02\n",
      " -3.53673324e-02  3.10559627e-02 -1.35544449e-01  4.62436192e-02\n",
      "  2.55149063e-02 -9.14995521e-02 -2.86395159e-02 -1.79143402e-08\n",
      " -1.08664641e-02 -6.86465716e-03 -4.26130891e-02  3.96252014e-02\n",
      " -2.69066636e-02  1.02078862e-01  9.47484821e-02  2.29625590e-02\n",
      "  1.40857752e-02 -2.46061552e-02  7.88934529e-02  1.66373886e-03\n",
      "  1.54373096e-02 -7.77945295e-02  1.40762357e-02 -5.43590710e-02\n",
      " -9.51998085e-02  1.15952283e-01  1.15115952e-03 -4.02605236e-02\n",
      " -1.10987304e-02  1.54442951e-01  1.65086538e-02 -3.54888253e-02\n",
      " -3.76385488e-02  5.25296815e-02  1.08523987e-01  1.00291111e-01\n",
      "  2.80253105e-02  8.26128293e-03 -4.35154587e-02  1.38984025e-02\n",
      " -3.12087610e-02 -8.58361870e-02 -1.35427201e-02 -1.75548822e-03\n",
      " -9.24221333e-03 -3.89467983e-04  4.45043445e-02 -2.06983481e-02\n",
      "  3.98022830e-02 -2.51020049e-03 -8.77528563e-02 -6.46544248e-02\n",
      "  4.01924103e-02 -1.81465149e-02 -4.97334935e-02 -2.84112189e-02\n",
      "  3.72506765e-04 -2.78107692e-02 -2.53050476e-02  7.03255087e-02\n",
      " -4.51048613e-02  1.99415963e-02  1.46342730e-02 -8.49403068e-02\n",
      " -4.66033146e-02 -1.73660684e-02 -5.08460924e-02  8.68468639e-03\n",
      "  6.41346350e-02 -6.08881377e-02  6.69399053e-02  2.88187861e-02]\n",
      "--------------------------------------------------\n",
      "Query: How is the consumer price index measured?\n",
      "Embedding: [ 2.43459940e-02 -1.18201124e-02 -5.10249920e-02  8.99115652e-02\n",
      "  6.17642980e-03  3.32221091e-02  4.66492139e-02  1.48509899e-02\n",
      "  5.93060255e-02 -5.16366065e-02  3.44562680e-02 -4.53715511e-02\n",
      "  1.31286811e-02 -2.70871166e-02 -1.09898239e-01 -9.85146016e-02\n",
      "  4.25832393e-03  3.97228561e-02  3.92174348e-02  2.22820286e-02\n",
      "  4.82768603e-02  2.40061898e-03  1.36468029e-02  5.73906954e-03\n",
      " -1.87537111e-02 -1.90426912e-02 -3.20200585e-02 -2.74866093e-02\n",
      "  1.36560453e-02  4.08121645e-02 -7.72027373e-02  3.05114817e-02\n",
      "  5.10790087e-02  5.03778309e-02 -3.21564041e-02 -4.20929380e-02\n",
      "  5.58375344e-02 -4.54866216e-02  2.61997003e-02  1.98990032e-02\n",
      "  4.20690365e-02 -4.17344160e-02 -5.64901829e-02  3.37102660e-03\n",
      "  4.27190922e-02  4.81313880e-04 -5.65938791e-03  8.04150626e-02\n",
      " -4.85934541e-02  1.06471844e-01 -1.05329663e-01  1.12583399e-01\n",
      " -2.84556020e-03 -1.53027037e-02  4.42474149e-02  6.20222390e-02\n",
      " -1.80893280e-02 -7.23754689e-02  7.68566430e-02  3.55315171e-02\n",
      "  3.84743512e-03 -6.13522856e-03 -1.13700554e-01  5.81975095e-02\n",
      "  9.36789364e-02 -2.12801341e-02 -2.47484818e-03 -8.26269016e-02\n",
      " -6.12997413e-02 -6.71733096e-02 -3.55198942e-02 -2.52918992e-02\n",
      "  1.80219230e-03 -6.21421374e-02 -1.37721347e-02 -2.93464642e-02\n",
      "  3.57476473e-02 -1.87709890e-02  2.83761546e-02  1.42417522e-02\n",
      "  9.17751528e-03 -5.40496036e-02 -1.25435437e-03  2.22992245e-02\n",
      "  9.01846066e-02 -6.75509200e-02  6.84834868e-02  4.37732274e-03\n",
      " -3.80733907e-02  2.19314452e-02  6.02931194e-02 -1.83896869e-02\n",
      " -1.13070615e-01  3.74795273e-02 -3.71580780e-03  4.64852862e-02\n",
      " -1.15635637e-02  1.99410543e-02  6.15577921e-02  4.15769080e-03\n",
      " -1.20220007e-02 -2.43604854e-02 -6.54303506e-02  6.28462881e-02\n",
      " -1.04387186e-01 -7.05776736e-02 -4.47959965e-03  1.28527433e-02\n",
      "  3.04029882e-02  5.27371913e-02 -8.57165530e-02 -2.22160630e-02\n",
      " -4.44047451e-02 -1.48090720e-02  3.33302878e-02 -3.24543305e-02\n",
      "  3.19773629e-02 -3.23783122e-02  1.24712862e-01  6.59898445e-02\n",
      "  2.31618434e-02 -1.79140223e-03  5.49854301e-02 -1.16456794e-02\n",
      " -2.86334865e-02  9.19950902e-02  1.32590458e-02 -5.57404176e-33\n",
      " -4.33480814e-02  3.18923965e-03  1.91025939e-02 -7.98828676e-02\n",
      " -1.17378488e-01  1.20092332e-02 -4.63095605e-02 -1.39801232e-02\n",
      "  5.82232475e-02  3.04776952e-02 -2.92833038e-02  8.87581334e-02\n",
      " -2.91826725e-02  6.72494993e-02  5.40729351e-02  6.12093089e-03\n",
      " -9.96503755e-02  8.99282992e-02  4.53523807e-02  2.13877540e-02\n",
      " -2.79540569e-02  3.85205746e-02  9.01906937e-02  3.68611813e-02\n",
      " -1.59123894e-02 -3.00399065e-02 -5.11942543e-02  2.47995444e-02\n",
      "  5.92002124e-02  2.56600305e-02  8.83250162e-02  1.82868517e-03\n",
      "  1.88753121e-02 -7.78521374e-02 -5.63868694e-02  3.72354276e-02\n",
      " -1.18325138e-02  6.88821077e-02 -3.94819565e-02 -5.21297520e-03\n",
      " -2.26962045e-02 -1.60056930e-02  5.54917455e-02 -2.92944368e-02\n",
      " -8.77172723e-02  1.56421252e-02 -2.90344954e-02 -8.43815878e-02\n",
      "  2.06647310e-02  3.60183604e-02 -7.39870742e-02 -2.89339256e-02\n",
      " -9.69010592e-03  1.87750123e-02  4.68256278e-03  2.48325355e-02\n",
      " -6.50857296e-03 -9.76340100e-02 -4.97756153e-02 -2.38399729e-02\n",
      " -7.93555677e-02  6.03993349e-02  6.40275627e-02  3.21725896e-03\n",
      " -2.59843953e-02  5.69646023e-02 -2.01098099e-02 -1.07175373e-02\n",
      "  2.05374788e-03  1.70997679e-02  4.98452806e-04  3.16431513e-03\n",
      "  3.61116379e-02 -4.40184064e-02 -4.65909205e-02  2.31828354e-02\n",
      " -6.48406819e-02  5.66623285e-02  1.24469846e-02  3.28905112e-03\n",
      " -5.77401817e-02  4.26652655e-02  1.37727141e-01 -8.25051218e-02\n",
      "  7.58048613e-03 -2.38677096e-02 -4.56624106e-02  4.82540205e-03\n",
      "  3.89792360e-02 -1.04986854e-01 -9.70454980e-03  4.80725802e-02\n",
      " -5.49353994e-02 -2.87574455e-02 -3.21919546e-02  2.93355213e-33\n",
      " -7.00523257e-02  6.60229474e-02  1.57185216e-02  1.16382465e-01\n",
      " -7.92113766e-02 -1.03486255e-01 -4.19127196e-02  1.19902194e-03\n",
      "  6.31878600e-02  3.84246558e-02 -8.22791383e-02  5.83267584e-02\n",
      " -2.89667808e-02  5.77279218e-02  3.98734957e-02  8.91561657e-02\n",
      "  2.93166656e-03 -2.28669625e-02  6.76870393e-03 -9.25605744e-02\n",
      " -8.55863318e-02  4.36403677e-02 -8.46765563e-03  5.39976768e-02\n",
      " -6.05839565e-02 -1.61050465e-02  8.26894566e-02 -1.50714498e-02\n",
      "  2.69266404e-02 -1.54757038e-01  3.23788039e-02 -1.28713816e-01\n",
      "  6.26844615e-02  3.04427501e-02  4.07699356e-03  5.24648465e-02\n",
      " -5.12730284e-03 -4.29508165e-02 -1.00874277e-02  6.63099140e-02\n",
      "  2.95037851e-02  3.69607806e-02 -3.29682231e-02  2.65891128e-03\n",
      "  7.20392689e-02 -2.21891534e-02  3.00195888e-02 -2.43327748e-02\n",
      "  2.45059896e-02 -2.37523131e-02  4.04635146e-02  7.36493021e-02\n",
      "  4.36048247e-02  1.59617793e-02 -8.95155817e-02  3.90526503e-02\n",
      "  1.53274396e-02  6.79201335e-02 -4.98441681e-02  1.57647487e-03\n",
      "  9.32644606e-02  4.60916124e-02 -3.60722318e-02  6.37184530e-02\n",
      "  2.11233236e-02  2.71035060e-02  6.60185143e-02 -1.36621043e-01\n",
      " -9.12850443e-03 -7.25010633e-02  2.29370687e-02 -4.27789278e-02\n",
      " -2.52704471e-02 -7.15259388e-02 -5.78775331e-02 -2.53648367e-02\n",
      " -2.27399617e-02  3.26103121e-02  1.42714358e-03  9.89160500e-03\n",
      " -2.69417763e-02  1.72259901e-02 -3.10651287e-02 -3.12292464e-02\n",
      " -6.08015023e-02 -8.82324502e-02  3.65088135e-02  1.67301390e-02\n",
      " -1.04339078e-01  5.17930649e-02 -7.00410828e-02  2.40943283e-02\n",
      " -5.71572334e-02 -1.06181763e-01 -1.91928260e-02 -1.60722422e-08\n",
      " -2.01706425e-03 -6.28880411e-02  1.22340256e-02  6.95941001e-02\n",
      " -3.76336859e-03  5.53400852e-02  4.68038209e-02 -1.40499361e-02\n",
      " -1.44087905e-02  3.42340954e-02  1.01758480e-01 -1.51254106e-02\n",
      " -7.89166391e-02  6.03893735e-02 -8.64737760e-03 -3.72303613e-02\n",
      "  1.28459670e-02  7.73318037e-02 -1.98302567e-02 -8.88250023e-03\n",
      "  2.52428162e-03  1.38053402e-01  4.27356437e-02 -2.02621855e-02\n",
      "  1.75356176e-02  7.72608966e-02  2.54889447e-02  5.26503325e-02\n",
      "  7.11919647e-03  4.67077978e-02  9.50961281e-03  9.69784800e-03\n",
      "  3.74457166e-02 -1.43470773e-02  1.68031119e-02  5.10944836e-02\n",
      " -4.72219773e-02 -6.07084017e-03 -1.65115353e-02 -1.33798197e-02\n",
      " -6.49527321e-03  2.41828933e-02 -3.90784070e-02  1.21701723e-02\n",
      "  8.61716419e-02  3.15966532e-02 -7.17674717e-02  7.40521774e-02\n",
      "  1.19154109e-02 -5.72250551e-03 -3.47474776e-03  5.38528105e-03\n",
      " -2.92455647e-02 -1.59038082e-02 -6.17389642e-02 -9.55062285e-02\n",
      " -9.35804378e-03 -9.66849364e-03 -2.61133350e-03  2.59912442e-02\n",
      "  5.34157790e-02 -3.20566893e-02 -5.68453260e-02 -5.30799804e-03]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, RobertaTokenizer, RobertaForSequenceClassification\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from time import time\n",
    "import numpy as np\n",
    "import psutil\n",
    "\n",
    "# Example search queries\n",
    "test_queries = [\n",
    "    \"What are the inflation rates in India?\",\n",
    "    \"CPI changes over the years?\",\n",
    "    \"How does the CPI affect rural and urban areas?\",\n",
    "    \"What is the inflation rate for different states?\",\n",
    "    \"How is the consumer price index measured?\"\n",
    "]\n",
    "\n",
    "# 1. Load the models (We will try BERT, RoBERTa, and Sentence-BERT)\n",
    "class SemanticSearchModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SemanticSearchModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        if model_name == 'bert-base-uncased':\n",
    "            self.model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "        elif model_name == 'roberta-base':\n",
    "            self.model = RobertaForSequenceClassification.from_pretrained('roberta-base')\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
    "        elif model_name == 'sentence-transformers/all-MiniLM-L6-v2':\n",
    "            self.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "            self.tokenizer = None  # Sentence-BERT doesn't need separate tokenizer\n",
    "\n",
    "    def forward(self, input_texts=None):\n",
    "        if self.tokenizer:\n",
    "            # Tokenize input text for BERT and RoBERTa models\n",
    "            encoding = self.tokenizer(input_texts, return_tensors='pt', padding=True, truncation=True)\n",
    "            input_ids = encoding['input_ids']\n",
    "            attention_mask = encoding['attention_mask']\n",
    "            output = self.model(input_ids, attention_mask=attention_mask)[0]\n",
    "        else:\n",
    "            # For Sentence-BERT, directly encode the texts to get embeddings\n",
    "            output = self.model.encode(input_texts)\n",
    "        return output\n",
    "\n",
    "# 2. Define a function to measure inference time and model size\n",
    "def get_model_size(model):\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    size = params * 4 / (1024**2)  # Convert params to MB\n",
    "    return size\n",
    "\n",
    "def measure_inference_time(model, texts):\n",
    "    start_time = time()\n",
    "    with torch.no_grad():\n",
    "        model(texts)\n",
    "    inference_time = time() - start_time\n",
    "    return inference_time\n",
    "\n",
    "# 3. Test the models and get performance stats\n",
    "models = ['bert-base-uncased', 'roberta-base', 'sentence-transformers/all-MiniLM-L6-v2']\n",
    "results = {}\n",
    "\n",
    "for model_name in models:\n",
    "    model = SemanticSearchModel(model_name)\n",
    "    \n",
    "    # For Sentence-BERT, we pass the texts directly\n",
    "    emb = model(test_queries)\n",
    "    \n",
    "    model_size = get_model_size(model)\n",
    "    inference_time = measure_inference_time(model, test_queries)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Model Size (MB)': model_size,\n",
    "        'Inference Time (s)': inference_time,\n",
    "        'Test Results': emb  # Embeddings for test queries\n",
    "    }\n",
    "\n",
    "# 4. Output the results\n",
    "for model_name, stats in results.items():\n",
    "    print(f\"Model: {model_name}\")\n",
    "    print(f\"Model Size (MB): {stats['Model Size (MB)']}\")\n",
    "    print(f\"Inference Time (s): {stats['Inference Time (s)']}\")\n",
    "    print(\"Test Results (Embeddings):\")\n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"Query: {query}\")\n",
    "        print(f\"Embedding: {stats['Test Results'][i]}\")\n",
    "        print('-' * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de249ff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîç Evaluating model: bert-base-uncased\n",
      "üì¶ Model Size: 417.64 MB\n",
      "‚ö° Inference Time (on corpus): 1.2504 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8324000239372253\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.7429999709129333\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.7874000072479248\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india year on year inflation rates % for g\n",
      "Similarity: 0.842199981212616\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.7551000118255615\n",
      "\n",
      "üîç Evaluating model: roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model Size: 475.49 MB\n",
      "‚ö° Inference Time (on corpus): 1.2978 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.9854000210762024\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.9745000004768372\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.9761000275611877\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.984000027179718\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.9747999906539917\n",
      "\n",
      "üîç Evaluating model: sentence-transformers/all-MiniLM-L6-v2\n",
      "üì¶ Model Size: N/A (pre-compiled) MB\n",
      "‚ö° Inference Time (on corpus): 0.3056 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8023999929428101\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.6980999708175659\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.5823000073432922\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8252999782562256\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.713699996471405\n",
      "\n",
      "üîç Evaluating model: sentence-transformers/paraphrase-MiniLM-L6-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - iitgn.ac.in\\Desktop\\Projects\\SmartSearchify-AI-Enabled-Semantic-Search-for-eSankhyiki-Portal\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model Size: N/A (pre-compiled) MB\n",
      "‚ö° Inference Time (on corpus): 0.0452 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8091999888420105\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.6218000054359436\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.5242999792098999\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india year on year inflation rates % for g\n",
      "Similarity: 0.847599983215332\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.7756999731063843\n",
      "\n",
      "üîç Evaluating model: sentence-transformers/all-mpnet-base-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - iitgn.ac.in\\Desktop\\Projects\\SmartSearchify-AI-Enabled-Semantic-Search-for-eSankhyiki-Portal\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--sentence-transformers--all-mpnet-base-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model Size: N/A (pre-compiled) MB\n",
      "‚ö° Inference Time (on corpus): 0.2689 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.7483000159263611\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.807699978351593\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.7250000238418579\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india year on year inflation rates % for g\n",
      "Similarity: 0.8540999889373779\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.7803999781608582\n",
      "\n",
      "üîç Evaluating model: sentence-transformers/multi-qa-MiniLM-L6-cos-v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - iitgn.ac.in\\Desktop\\Projects\\SmartSearchify-AI-Enabled-Semantic-Search-for-eSankhyiki-Portal\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--sentence-transformers--multi-qa-MiniLM-L6-cos-v1. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model Size: N/A (pre-compiled) MB\n",
      "‚ö° Inference Time (on corpus): 0.0416 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8515999913215637\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.6211000084877014\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.5223000049591064\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8173999786376953\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.7167999744415283\n",
      "\n",
      "üîç Evaluating model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\OneDrive - iitgn.ac.in\\Desktop\\Projects\\SmartSearchify-AI-Enabled-Semantic-Search-for-eSankhyiki-Portal\\env\\lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Asus\\.cache\\huggingface\\hub\\models--sentence-transformers--paraphrase-multilingual-MiniLM-L12-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ Model Size: N/A (pre-compiled) MB\n",
      "‚ö° Inference Time (on corpus): 0.0855 s\n",
      "üìà Test Results:\n",
      "\n",
      "Query: inflation in India based on CPI\n",
      "Top Match: all india inflation rate based on cpi base 201\n",
      "Similarity: 0.8590999841690063\n",
      "\n",
      "Query: rural and urban consumer price index\n",
      "Top Match: general cpi for states for rural urban and com\n",
      "Similarity: 0.48080000281333923\n",
      "\n",
      "Query: general price index for states\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.6478999853134155\n",
      "\n",
      "Query: inflation rates for all India\n",
      "Top Match: all india year on year inflation rates % for g\n",
      "Similarity: 0.8781999945640564\n",
      "\n",
      "Query: state-wise inflation trend\n",
      "Top Match: year on year inflation rates % of major states\n",
      "Similarity: 0.8495000004768372\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from transformers import BertTokenizer, BertModel, RobertaTokenizer, RobertaModel\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from time import time\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Dummy corpus and test queries\n",
    "search_texts = [\n",
    "    \"all india year on year inflation rates % for g\",\n",
    "    \"all india inflation rate based on cpi base 201\",\n",
    "    \"year on year inflation rates % of major states\",\n",
    "    \"general cpi for states for rural urban and com\",\n",
    "    \"all india general group and sub group level cp\"\n",
    "]\n",
    "\n",
    "test_queries = [\n",
    "    \"inflation in India based on CPI\",\n",
    "    \"rural and urban consumer price index\",\n",
    "    \"general price index for states\",\n",
    "    \"inflation rates for all India\",\n",
    "    \"state-wise inflation trend\"\n",
    "]\n",
    "\n",
    "# Define SemanticSearchModel wrapper\n",
    "class SemanticSearchModel(nn.Module):\n",
    "    def __init__(self, model_name='bert-base-uncased'):\n",
    "        super(SemanticSearchModel, self).__init__()\n",
    "        self.model_name = model_name\n",
    "        self.sentence_bert = False\n",
    "\n",
    "        if model_name == 'bert-base-uncased':\n",
    "            self.model = BertModel.from_pretrained(model_name)\n",
    "            self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        elif model_name == 'roberta-base':\n",
    "            self.model = RobertaModel.from_pretrained(model_name)\n",
    "            self.tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "        elif model_name.startswith(\"sentence-transformers\"):\n",
    "            self.model = SentenceTransformer(model_name)\n",
    "            self.tokenizer = None\n",
    "            self.sentence_bert = True\n",
    "\n",
    "    def get_embeddings(self, texts):\n",
    "        if self.sentence_bert:\n",
    "            return self.model.encode(texts, convert_to_tensor=True)\n",
    "        else:\n",
    "            tokens = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model(**tokens)\n",
    "            embeddings = outputs.last_hidden_state.mean(dim=1)  # Mean pooling\n",
    "            return embeddings\n",
    "\n",
    "# Get model size\n",
    "def get_model_size(model):\n",
    "    if isinstance(model, SentenceTransformer):\n",
    "        return \"N/A (pre-compiled)\"\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    return round(params * 4 / (1024 ** 2), 2)  # in MB\n",
    "\n",
    "# Inference time\n",
    "def measure_inference_time(model, texts):\n",
    "    start = time()\n",
    "    model.get_embeddings(texts)\n",
    "    return round(time() - start, 4)\n",
    "\n",
    "# Compare a single test query with corpus and return best match\n",
    "def get_best_matches(model, query, corpus_embeddings, corpus_texts, top_k=1):\n",
    "    query_embedding = model.get_embeddings([query])\n",
    "    scores = cosine_similarity(query_embedding.cpu(), corpus_embeddings.cpu())[0]\n",
    "    top_idx = np.argsort(scores)[::-1][:top_k]\n",
    "    return [(corpus_texts[i], scores[i]) for i in top_idx]\n",
    "\n",
    "# Models to evaluate\n",
    "model_names = [\n",
    "    'bert-base-uncased',\n",
    "    'roberta-base',\n",
    "    'sentence-transformers/all-MiniLM-L6-v2',\n",
    "    'sentence-transformers/paraphrase-MiniLM-L6-v2',\n",
    "    'sentence-transformers/all-mpnet-base-v2',\n",
    "    'sentence-transformers/multi-qa-MiniLM-L6-cos-v1',\n",
    "    'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'\n",
    "]\n",
    "\n",
    "# Run benchmark\n",
    "for model_name in model_names:\n",
    "    print(f\"\\nüîç Evaluating model: {model_name}\")\n",
    "    model = SemanticSearchModel(model_name)\n",
    "\n",
    "    model_size = get_model_size(model.model)\n",
    "    inference_time = measure_inference_time(model, search_texts)\n",
    "    corpus_embeddings = model.get_embeddings(search_texts)\n",
    "\n",
    "    print(f\"üì¶ Model Size: {model_size} MB\")\n",
    "    print(f\"‚ö° Inference Time (on corpus): {inference_time} s\")\n",
    "    print(\"üìà Test Results:\")\n",
    "\n",
    "    for query in test_queries:\n",
    "        match_text, score = get_best_matches(model, query, corpus_embeddings, search_texts)[0]\n",
    "        print(f\"\\nQuery: {query}\")\n",
    "        print(f\"Top Match: {match_text}\")\n",
    "        print(f\"Similarity: {round(score, 4)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb65683",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
